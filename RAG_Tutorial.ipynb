{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thPS0JtZVTII"
      },
      "source": [
        "<img src=\"https://wdsa-ccwi2024.it/wp-content/uploads/2023/02/logo-2-175x127.png\" alt=\"WDSA/CCWI Logo\" width=\"150\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvsz7txiX4Dm"
      },
      "source": [
        "# Basic of Retrieval Augmented Generation (RAG) with Huggingface and Langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1kRIDT97wTh"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kih21u1tyr-I"
      },
      "source": [
        "\n",
        "This notebook demonstrates the basics of RAG and its use for improving the capabilities of Large Language Models (LLM). RAG provides relevant and updated context without the need of fine-tuning. RAG also reduces hallucinations, largely increasing the usability of LLMs in real-world applications. We will show the benefits of RAG by i) using it to \"teach\" an LLM about the *Water Network Tool for Resilience* (WNTR) toolbox, and ii) creating a quick application to chat with online blogs.\n",
        "\n",
        "This notebook is partially based on [this](https://huggingface.co/learn/cookbook/en/rag_zephyr_langchain) and [this](https://python.langchain.com/v0.1/docs/use_cases/question_answering/chat_history/) contributions.\n",
        "\n",
        "\n",
        "\n",
        "**What is RAG?**\n",
        "\n",
        "RAG is a popular approach to address the issue of a LLMs not being aware of specific content due to said content not being in its training data, or hallucinating even when it has seen it before. Such specific content may be proprietary, sensitive, or recent and updated often. If you have sufficient computing capabilities, and your data is static and does not change regularly, you may consider fine-tuning the LLM. In many cases, however, fine-tuning can be costly, and, when done repeatedly (e.g. to address data drift), leads to \"model shift\". This is when the model's behavior changes in ways that are not desirable.\n",
        "\n",
        "**RAG (Retrieval Augmented Generation)** does not require model fine-tuning. Instead, RAG works by providing an LLM with additional context that is retrieved from relevant data so that it can generate a better-informed response.\n",
        "\n",
        "Here's a quick illustration of how it works:\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/rag-diagram.png\" alt=\"RAG Diagram\" width=\"600\"/>\n",
        "\n",
        "\n",
        "* The external data is converted into *embedding* vectors with a separate **embeddings model**. These models are smaller language models which translate parts documents into vectors (a format AI can understand, similar to how tokens are transformed in a LLM). The vectors capture the *semantic meaning* of the text. These vectors are stored in a **vector database**. Embedding models are typically small, so updating the embedding vectors on a regular basis is faster, cheaper, and easier than fine-tuning a model.\n",
        "\n",
        "* When a **query** is made, the embeddings model converts this query into its embedding vector, which is then used to search the vector database for documents with similar embeddings. This retrieval process ensures that the most relevant documents related to the query are identified.\n",
        "\n",
        "* The retrieved documents, which are similar to the query, are then combined with the original query to form a new, more informative **prompt**. This prompt provides the LLM with the necessary context to generate a response that is both accurate and relevant to the query.\n",
        "\n",
        "* The LLM processes the combined prompt (query + similar documents) and generates a **response** that benefits from the augmented context provided by the retrieved documents.\n",
        "\n",
        "* RAG also gives you the opportunity to swap your LLM for a more powerful one when it becomes available, or switch to a smaller distilled version, should you need faster inference.\n",
        "\n",
        "This approach leverages the strengths of both *retrieval-based* and *generation-based* models, ensuring that responses are grounded in specific, relevant data while also being generated in a coherent and contextually appropriate manner.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRDdkw7LPni6"
      },
      "source": [
        "## Part 1: RAG with WNTR documentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ee3GTQa8MS7"
      },
      "source": [
        "### Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJCE-Ctd8N2R"
      },
      "source": [
        "\n",
        "We will implement a basic RAG pipeline to provide knowledge of the [Water Network Tool for Resilience (WNTR) toolbox](https://github.com/USEPA/WNTR) to an LLM. We will use [HuggingFace](https://huggingface.co/) to download and access the LLM and embeddings models. We will use [LangChain](https://www.langchain.com/) to build the RAG pipeline.\n",
        "\n",
        "As you may know, **WNTR** is an open-source Python package developed by the U.S. EPA and Sandia National Laboratories to simulate and analyze the resilience of water distribution networks against disruptive events like natural disasters and infrastructure failures. It integrates hydraulic and water quality simulations, damage estimates, and resilience metrics, and it provides a Python interface to EPANET.\n",
        "\n",
        "**HuggingFace** is a leading AI platform and community that provides open-source tools and resources for developing, sharing, and using AI models. It is best known for its *Transformers* library, which offers a vast collection of pre-trained models for various tasks like NLP, computer vision, and audio processing, simplifying the integration of sophisticated models into applications. The platform also features the Hugging Face Hub, where users can collaborate on models, datasets, and applications, fostering a collaborative environment for AI development.\n",
        "\n",
        "**LangChain** is a framework that simplifies the development of applications powered by LLMs. It is made by modular components for handling different tasks such as prompt management, document loading, preprocessing, interfacing with vector databases and other functionalities enabling building complex RAG pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eTWoN14FlWU"
      },
      "source": [
        "### Install all required libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZYYbljIGBjG"
      },
      "source": [
        "The following instructions will install all the necessary libraries for this tutorial. You do not need to pay attention to it, but you must run all these cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdrpdEhcF4xw"
      },
      "outputs": [],
      "source": [
        "# installing libraries for the tutorial, these will be discussed later; feel free to check online if you need more info on each of them\n",
        "!pip install --upgrade --quiet torch transformers accelerate bitsandbytes transformers sentence-transformers faiss-gpu langchain langchain-community langchain-huggingface GitPython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQjJf_z_FzAo"
      },
      "outputs": [],
      "source": [
        "# Override the default locale setting to ensure UTF-8 encoding is used.\n",
        "# The locale is a set of parameters that defines the user's language,\n",
        "# country, and any special variant preferences, affecting how data is presented.\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBhqEhW7IGPJ"
      },
      "source": [
        "### LLM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fozsytC8P9CP"
      },
      "source": [
        "In this notebook, we'll demonstrate RAG using \"small\" open LLMs. You can choose between HuggingFace's `Zephyr-7b` (7B parameters) or Microsoft's `Phi-3 Mini-128k-Instruct` (3.8B parameters). You can add other small models available on [HuggingFace](https://huggingface.co/models). Due to resource constraints, we suggest to start with Phi-3, a powerful model for its size. Yet, we will apply quantization techniques to fit the model on our hardware. The `128k` identifies the length of the context window.\n",
        "\n",
        "The code below performs the setup for using a pretrained language model with quantization and tokenization. *Quantization* is a technique that reduces the precision of the model's weights, decreasing memory usage and increasing inference speed without significantly sacrificing accuracy. *Tokenization* is the process of converting text into *tokens*, breaking down the input into smaller units such as words or subwords. Tokens still need to be converted into embeddings that the model can understand and process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZPgDAvJKE1Y"
      },
      "source": [
        "To access HuggingFace's resources, we need to login and authenticate our account. You can do this with the simple lines of code below, and by pasting a \"WRITE\" token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAuL4S-gS3ca"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqxradE_Gow1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# Choose the model to be used. Uncomment the desired model_name line.\n",
        "#model_name = 'HuggingFaceH4/zephyr-7b-beta'\n",
        "model_name = 'microsoft/Phi-3-mini-128k-instruct'\n",
        "\n",
        "# Configuration for loading the model with 4-bit quantization\n",
        "# For more info check https://huggingface.co/docs/transformers/en/main_classes/quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load the model with the specified quantization configuration\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
        "\n",
        "# Load the tokenizer corresponding to the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ocmv_dM3XIbh"
      },
      "source": [
        "### Creating the LLM Chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ9FcY0UXaXb"
      },
      "source": [
        "This code snippet demonstrates how to create a language model **chain** (`llm_chain`) using HuggingFace and LangChain. The process involves setting up a text generation pipeline with HuggingFace, integrating it with LangChain, and defining a prompt template to structure the input for the language model. The final `llm_chain` is constructed by combining the prompt template, language model, and output parser, allowing for streamlined input-output processing.\n",
        "\n",
        "* A text generation pipeline is a component in HuggingFace that enables the generation of text based on a given prompt, using a specified model and tokenizer. It allows for customization of various parameters like temperature, repetition penalty, and maximum number of tokens to generate coherent and contextually relevant text. *Temperature* is a value between 0 and 2 that controls the randomness of the text generation; lower values make the output more deterministic, while higher values increase diversity. For RAG we usually select very low values. *Repetition penalty* reduces the likelihood of repetitive sequences in the generated text, ensuring more varied and natural responses.\n",
        "\n",
        "* On the other hand, LangChain pipelines, known as chains (hence the name) are created using the `|` operator, known as the *pipe* operator. This chains together different components (like prompt templates, language models, and output parsers) into a single, cohesive workflow. The pipe operator originates from Unix/Linux, where it is used to pass the output of one command as input to another, enabling the construction of complex command sequences. LangChain pipelines differ from HuggingFace pipelines as they enable the integration of various processing steps and components in a modular and extensible manner, facilitating complex input-output transformations and handling within a single framework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9cp56BXKuFY"
      },
      "source": [
        "First, we create the HuggingFace pipeline..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZC6TmoQQIc_f"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import pipeline\n",
        "\n",
        "# Create a text generation pipeline using the specified model and tokenizer\n",
        "text_generation_pipeline = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    temperature=0.1,\n",
        "    do_sample=True,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True,\n",
        "    max_new_tokens=400,\n",
        ")\n",
        "\n",
        "# Wrap the pipeline in a Langchain HuggingFacePipeline object\n",
        "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88urd3yLY3Cz"
      },
      "source": [
        "... next, the code below create a LangChain pipeline from the HuggingFace one. You can see that we are using a structured prompt template with *placeholders* and *roles*.\n",
        "\n",
        "* Placeholders like `{context}` and `{question}` in the template will be replaced with actual values when used. `{context}` provides the background information to the model to help generate a relevant answer. `{question}` is the specific query the user wants the model to answer. [These placeholders](https://www.w3schools.com/python/ref_string_format.asp) are general Python features, not specific to LLMs, allowing for flexible and dynamic input handling.\n",
        "\n",
        "* As for the roles, `system` usually defines the structure and rules of the interaction (e.g., prompt template); `user` is the entity asking the question, providing `context` and `question`; `assistant` is the AI model generating the response based on the provided context and question.\n",
        "\n",
        "We prefer our prompts to be structured to ensure the model receives all necessary information in a consistent format, improving the accuracy and relevance of its responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_nT02kNY2X9"
      },
      "outputs": [],
      "source": [
        "# Define the template for the prompt, including placeholders for context and question\n",
        "prompt_template = \"\"\"\n",
        "<|system|>\n",
        "Answer the question based on your knowledge. Use the following context to help:\n",
        "\n",
        "{context}\n",
        "\n",
        "</s>\n",
        "<|user|>\n",
        "{question}\n",
        "</s>\n",
        "<|assistant|>\n",
        "\n",
        " \"\"\"\n",
        "# Create a PromptTemplate object with specified input variables and template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=prompt_template,\n",
        ")\n",
        "\n",
        "# Combine the prompt template, language model, and output parser into a single chain\n",
        "llm_chain = prompt | llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zl-zEqUrjeWA"
      },
      "source": [
        "### Testing the LLM Chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKs6d1Hia6E-"
      },
      "source": [
        "Now is time to test our LLM chain. We ask two simple questions on WNTR to verify what the LLM already knows about the topic... it does not seem like it knows a lot. We also time the responses to check how long it takes to produce them, given the constrained computational resources of Google Colab. Lastly, we use the \"pretty print\" module `pprint` to better visualize the answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpzQdvXmIopT"
      },
      "outputs": [],
      "source": [
        "question = \"What does WNTR stand for in the context of water distribution systems analysis?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zb6nvy2NJ5wv"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "llm_response = llm_chain.invoke({\"context\":\"\", \"question\": question})\n",
        "end_time = time.time()\n",
        "\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Execution time: {execution_time} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6z37AP7W8hC"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "pprint.pprint(llm_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blPlyOyobXga"
      },
      "outputs": [],
      "source": [
        "question = \"What can you tell me about the Pressure Dependent Demand model of EPANET?\"\n",
        "start_time = time.time()\n",
        "llm_response = llm_chain.invoke({\"context\":\"\", \"question\": question})\n",
        "end_time = time.time()\n",
        "\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Execution time: {execution_time} seconds\")\n",
        "pprint.pprint(llm_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TstKXwIbrP0"
      },
      "source": [
        "The output above demonstrates how we get both the input and the output when querying our LangChain pipeline. In this example, we can see that the `context` is empty because we only provided a question. The answer appears after the assistant role identifier, demarcating where the model's response begins.\n",
        "\n",
        "It is evident that some post-processing is necessary to extract only the answer, similar to what users expect from tools like ChatGPT. However, ChatGPT is more than just a language model; it is a refined application. Using LangChain and other tools, we can create similar applications, but this is beyond the scope of this basic tutorial.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8po01vMWzXL"
      },
      "source": [
        "### Load the data for RAG\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y1gnzt9kpE6"
      },
      "source": [
        "We use the `GitLoader` from the `langchain_community.document_loaders` module to clone the `WNTR` GitHub repository locally and extract all the `.rst` files from a specified directory within the repository. `.rst` files are [reStructuredText](https://en.wikipedia.org/wiki/ReStructuredText) files, primarily used for documentation in Python packages. They are readable by humans and can be converted to various formats like HTML and PDF.\n",
        "The `file_to_load` function is used to filter and ensure that only the `.rst` files located directly in the `./WNTR/documentation` directory are loaded, excluding any subdirectories. One great thing about LangChain is that the developers and the community provide [many loaders](https://python.langchain.com/v0.2/docs/integrations/document_loaders/) for different types of sources, including online websites and PDFs.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8iiUpgfGZFI"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import GitLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SV6mfIFCkIDm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def file_to_load(file_path, base_directory='./WNTR/documentation'):\n",
        "    \"\"\"\n",
        "    Returns True if the file is a .rst file in the specified base directory, without checking subfolders..\n",
        "\n",
        "    Returns:\n",
        "    bool: True if the file should be loaded, False otherwise.\n",
        "    \"\"\"\n",
        "    # Check if the file is in the specified base directory and ends with .rst\n",
        "    if os.path.dirname(file_path) == base_directory:\n",
        "      return file_path.endswith('.rst')\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDAkh2atX4Dq"
      },
      "outputs": [],
      "source": [
        "loader = GitLoader(\n",
        "    clone_url=\"https://github.com/reganmurray/WNTR/\",\n",
        "    repo_path=\"./WNTR/\",\n",
        "    branch=\"master\",\n",
        "    file_filter=file_to_load,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChxNj7woW_u9"
      },
      "outputs": [],
      "source": [
        "# show the downloaded folder\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHG2RzJMG1HF"
      },
      "outputs": [],
      "source": [
        "# load the data\n",
        "docs = loader.load()\n",
        "print(len(docs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8J2fpdxHbWe"
      },
      "outputs": [],
      "source": [
        "# let's show the acknowledgements document\n",
        "pprint.pprint(docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2ik8QG1nSrn"
      },
      "source": [
        "### Document Chunking, Embeddings, and Vector Database Creation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When doing RAG with large documents, it's essential to chunk them into smaller pieces. This is necessary because many embedding models and vector databases have limitations on the maximum input size they can handle. Chunking ensures that each segment of the document can be processed efficiently and that the embeddings capture the relevant context without exceeding these size limitations.\n",
        "\n",
        "To create document chunk embeddings, we'll use the `HuggingFaceEmbeddings` class and the [`BAAI/bge-base-en-v1.5`](https://huggingface.co/BAAI/bge-base-en-v1.5) embeddings model. There are many other embedding models available on the Hub, and you can keep an eye on the best-performing ones by checking the [Massive Text Embedding Benchmark (MTEB) Leaderboard](https://huggingface.co/spaces/mteb/leaderboard).\n",
        "\n",
        "*Vector databases* store numerical representations (i.e., the embeddings) of data like text and images. They create an index to quickly find similar vectors to a query vector. This is done using special algorithms and data structures. Vector databases help find the most similar vectors to a query by measuring *distances* like cosine similarity or Euclidean distance in multidimensional spaces.\n",
        "\n",
        "For the vector database, we will use `FAISS`, a library developed by Facebook AI, which offers efficient similarity search and clustering of dense vectors, making it a popular library. We will access both the embeddings model and FAISS via the LangChain API."
      ],
      "metadata": {
        "id": "wMpzkMfO8VTr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "115-5ilaX4Dr"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Chunking documents into smaller pieces to ensure they fit the input size limits of the embedding model\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=32)\n",
        "\n",
        "chunked_docs = splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fk1rjE1KX4Dr"
      },
      "outputs": [],
      "source": [
        "# len(chunked_docs)>len(docs)\n",
        "print(len(chunked_docs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixmCdRzBQ5gu"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Using the HuggingFaceEmbeddings with FAISS to create a vector database from the chunked document embeddings\n",
        "db = FAISS.from_documents(chunked_docs,\n",
        "                          HuggingFaceEmbeddings(model_name='BAAI/bge-base-en-v1.5'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iCgEPi0nnN6"
      },
      "source": [
        "We need a way to return(retrieve) the documents given a query. For that, we'll use the `as_retriever` method using the `db` as a backbone:\n",
        "- `search_type=\"similarity\"` means we want to perform similarity search between the query and documents\n",
        "- `search_kwargs={'k': 5}` instructs the retriever to return top 5 results.\n",
        "\n",
        "There are other types of strategies for searching, such as [Maximal Marginal Relevance](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/mmr/) which aims to return results that are both relevant and diverse. Sometimes, you might want to retrieve very contrasting chunks to cover a broader spectrum of information related to the query. This can be particularly useful when you want to avoid redundancy and ensure diverse perspectives in the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBTreCQ9noHK"
      },
      "outputs": [],
      "source": [
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={'k': 5}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgEhlISJpTgj"
      },
      "source": [
        "The vector database and retriever are now set up. Now, we can finalize our RAG chain by combining the `llm_chain` with the retriever."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAz3qxyYX4Dr"
      },
      "source": [
        "### Creating the RAG Chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3BYEU3va_u7"
      },
      "source": [
        "Now that we created the retriever from the vector database, we can use it in a chain with the language model to generate responses based on retrieved contexts. We will build the chain using the `RunnablePassthrough` Class of LangChain, that simply passes the input (i.e., the question) through without any changes. It is used here to structure the input before passing it to the language model chain `llm_chain` we created before. Note that the `context` field is now filled by the retriever, which will fetch relevant information or documents based on the input query.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rI3YNp9Xl4s"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Set up the RAG chain\n",
        "rag_chain = (\n",
        " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | llm_chain\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsCOhfDDXpaS"
      },
      "source": [
        "### Testing the RAG chain\n",
        "\n",
        "Let's see the difference RAG makes in generating answers to the library-specific questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZpNA3o10H10"
      },
      "outputs": [],
      "source": [
        "question = \"What does WNTR stand for in the context of water distribution systems analysis?\"\n",
        "rag_response = rag_chain.invoke(question)\n",
        "pprint.pprint(rag_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h32ubPlbh4kR"
      },
      "source": [
        "If you scroll at the bottom of the previous printed text, you will see that the LLM now returns an accurate answer! You will also see at the top the chunks of documents retrieved by the RAG chain to produce such an answer, grounding the \"hallucinations\" of the LLM. However, now that we retrieve all this information, it is hard to find just the output that we want, i.e., the actual answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rs3Yj3qrfpvd"
      },
      "source": [
        "### Parsing the output of the LLM\n",
        "\n",
        "Indeed, *parsing* the output of an LLM is crucial because the raw output is often not immediately usable. With LangChain, we can implement various parsers to clean and structure this output. Here, we create a simple parser that removes everything except the assistant's response and we add it at the end of the chain. We do so by creating a new subclass from the `BaseOutputParser` Class of LangChain. You can find a quick recap of these basic Object Oriented Programming concepts in Python [here](https://www.geeksforgeeks.org/create-a-python-subclass/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ISuCkt0fp1z"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import BaseOutputParser\n",
        "\n",
        "class ReturnOnlyAssistantText(BaseOutputParser):\n",
        "    def parse(self, text: str):\n",
        "        # Find the position of the substring\n",
        "        index = text.find('<|assistant|>')\n",
        "        if index == -1:\n",
        "            # If the substring is not found, return an empty string or handle it as needed\n",
        "            return \"\"\n",
        "        # Return the text after the substring\n",
        "        return text[index + len('<|assistant|>'):]\n",
        "\n",
        "# Set up the RAG chain with parser\n",
        "rag_chain = (\n",
        " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | llm_chain | ReturnOnlyAssistantText()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKv3221agsfn"
      },
      "outputs": [],
      "source": [
        "question = \"What can you tell me about the Pressure Dependent Demand model of EPANET?\"\n",
        "rag_response = rag_chain.invoke(question)\n",
        "pprint.pprint(rag_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9O31lZlkDJy"
      },
      "source": [
        "As you can see, even adding a simple parser makes the output of the LLM more usable. This is important in real applications to ensure consistency, accuracy, and relevance in the data being processed and utilized by downstream systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPXd0Bg2nrMW"
      },
      "source": [
        "## Part 2: Conversational RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgJZjUb2s2PO"
      },
      "source": [
        "By now you should be familiar with RAG: the approach uses a retriever to fetch relevant documents based on a query and a generator to produce a natural language response using the retrieved documents. This method ensures that the LLM responses are both accurate and contextually relevant.\n",
        "\n",
        "Now we are going to look into *conversational RAG*, which maintains context over multiple interactions, making it more interesting and effective than single-shot retrieval. This approach enables chat systems to interact more naturally, much like you would with a human, and is fundamental in creating intelligent assistants. Everyone should be very familiar with this concept by using interfaces like *ChatGPT*, *Bing*, or *Gemini*. While we will keep it simple here, we will demonstrate the basics of building such a system using LangChain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxzrxRvzs2PO"
      },
      "source": [
        "Conversational RAG is illustrated by the following flowchart:\n",
        "\n",
        "<img src=\"https://python.langchain.com/v0.1/assets/images/conversational_retrieval_chain-5c7a96abe29e582bc575a0a0d63f86b0.png\" alt=\"RAG Diagram\" width=\"1000\"/>\n",
        "\n",
        "The system is divided into two main components: the `history aware retriever` and the `question answer chain`.\n",
        "\n",
        "The process starts with the user's input query, which is combined with the chat history to maintain context. This ensures that the conversation remains coherent and relevant to previous interactions. For the history aware retriever, a prompt is created to contextualize the query. This prompt is processed by the LLM to produce a contextualized query, which is more specific and tailored to the ongoing conversation.\n",
        "\n",
        "In the subsequent question answer chain, the contextualized query is then used to fetch relevant documents from the document store. The retrieved documents are then used to create a prompt that aims to answer the user's question. This new prompt is again processed by the LLM, which generates the final answer. The answer is then returned to the user, completing the cycle.\n",
        "\n",
        "We are essentially using two chains, linked together, each of them making use of an LLM. The first chain contextualizes the query, and the second chain generates the final answer based on the retrieved documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkHRvTkhqbN8"
      },
      "source": [
        "To illustrate how to build a basic system with LangChain, we will \"chat\" with the LLM with respect to the content of a [blog](https://blog.dhigroup.com/best-practices-to-address-aging-urban-water-infrastructure/) from DHI on best practices to address aging urban water infrastructure. This example will also showcase how easy it is to access data from websites in LangChain using the `WebBaseLoader`.e capabilities of LLMs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYqpCHZ8s2PO"
      },
      "source": [
        "> Disclaimer: We are using a pretty basic LLM, which is also quantized. Therefore, performance might not be as good as expected. For instance, if you are familiar with *ChatGPT*, you might notice differences in response quality. In general, simple models can perform well in these contexts too, but they require more sophisticated engineering to achieve good performance. This is beyond the scope of this course.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnBdf01ls2PO"
      },
      "source": [
        "### Get the documents from the web and create the retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHpaXJnM9fbJ"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoPmj1Ypr6wo"
      },
      "outputs": [],
      "source": [
        "# Load, chunk and index the contents of the blog.\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://blog.dhigroup.com/best-practices-to-address-aging-urban-water-infrastructure/\",)\n",
        ")\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afoeKqMwtP3z"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "db = FAISS.from_documents(chunks,\n",
        "                          HuggingFaceEmbeddings(model_name='BAAI/bge-base-en-v1.5'))\n",
        "\n",
        "# Retrieve and generate using the relevant snippets of the blog.\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={'k': 3})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnusWlBIs2PO"
      },
      "source": [
        "### Test the LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_rEkh4ts2PO"
      },
      "source": [
        "First, we check the basic LLM response with respect to a specific aspect of the blog. We can see that the answer is pretty generic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grmOQA-T9zWd"
      },
      "outputs": [],
      "source": [
        "llm_response = llm_chain.invoke({\"context\":\"\", \"question\": \"What does creating an holistic plan entail for municipalities?\"})\n",
        "pprint.pprint(llm_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ykd79pms2PP"
      },
      "source": [
        "### Creation of the conversational RAG pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTLjiOlBs2PP"
      },
      "source": [
        "Now we create our conversational RAG pipeline.\n",
        "\n",
        "First we create the `history_aware_retriever`. We define a system prompt  to contextualize the user's query, instructing the model to reformulate the latest user question into a standalone question that can be understood without referring to the chat history.\n",
        "\n",
        "While it may appear counterintuitive to create standalone questions when chat history is available, this process improves clarity, reduces ambiguity, and enhances retrieval accuracy, ensuring the system fetches relevant information more effectively and generates more precise responses.\n",
        "\n",
        "A `ChatPromptTemplate` is created using this system prompt, along with a placeholders for the chat history (i.e., `MessagesPlaceholder`) and user input. Finally, the `create_history_aware_retriever` function combines the LLM, the retriever, and the contextualized query prompt to create a retriever that maintains context over multiple interactions. This setup allows the system to handle user queries more effectively by considering the conversation history.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9xF8uHNtNek"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "\n",
        "# Define the system prompt to contextualize the query.\n",
        "contextualize_q_system_prompt = (\n",
        "    \"\"\"Given a chat history and the latest human question \\\n",
        "    which might reference context in the chat history, \\\n",
        "    formulate a standalone question which can be understood \\\n",
        "    without the chat history. Do NOT answer the question \\\n",
        "    just reformulate the human question if you can \\\n",
        "    and otherwise return it as it is.\"\"\"\n",
        ")\n",
        "\n",
        "# Create a ChatPromptTemplate using the system prompt, chat history, and user input\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create a history-aware retriever using the LLM, retriever, and contextualize query prompt\n",
        "# This combines the contextualization step with the retrieval process\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm, retriever, contextualize_q_prompt\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfXhuJFLs2PP"
      },
      "source": [
        "Now we create the `question_answer_chain`. To do so, we define a system prompt to instruct the assistant on how to answer questions concisely using retrieved context. A different `ChatPromptTemplate` is created with this system prompt, incorporating placeholders for the chat history and user input. This template ensures that the model uses relevant context and maintains conversation continuity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jm1_OnqCtNhC"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "# Define the system prompt for the question-answering task\n",
        "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
        "Use the following pieces of retrieved context to answer the question. \\\n",
        "If you don't know the answer, just say that you don't know. \\\n",
        "Use three sentences maximum and keep the answer concise.\n",
        "\n",
        "<START CONTEXT>:{context}<END CONTEXT>\"\"\"\n",
        "\n",
        "# Create a ChatPromptTemplate using the QA system prompt, chat history, and user input\n",
        "qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", qa_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create a chain that combines documents using the specified LLM and QA prompt\n",
        "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDb-m04ds2PP"
      },
      "source": [
        "Finally, the `rag_chain` is formed by combining the `history_aware_retriever` and the `question_answer_chain`. This setup enables the system to effectively retrieve relevant documents and generate contextually aware answers to user queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sj9ZqtzVs2PP"
      },
      "outputs": [],
      "source": [
        "# Create the retrieval chain by combining the history-aware retriever and the QA chain\n",
        "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xokUVIrys2PP"
      },
      "source": [
        "### Test the chat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFHPMlNlPdrr"
      },
      "source": [
        "We can finally text the chat! While creating a full-fledged conversational user interface is beyond the scope of this tutorial, we can simulate the conversation using the `AIMessage` and `HumanMessage` classes from Langchain, as illustrated below. As you can see, the three questions build on eachother and the responses provided by the LLM at each stage.\n",
        "\n",
        "To focus on the Q/A pairs without printing the context and other details, we define and use a function `extract_answers` to extract and display only the questions and answers.\n",
        "\n",
        "> Note that the model, being simple and quantized, might have the tendency to hallucinate, potentially asking new intermediate questions that were not actually posed by the user.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QupVDtRZtNjG"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "\n",
        "# Initialize an empty chat history\n",
        "chat_history = []\n",
        "\n",
        "# First question from the user\n",
        "question = \"What does IUWM stand for?\"\n",
        "\n",
        "# Invoke the RAG chain to get the AI response for the first question\n",
        "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
        "\n",
        "# Update chat history with the human question and AI response\n",
        "chat_history.extend(\n",
        "    [\n",
        "        HumanMessage(content=question),\n",
        "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Second question\n",
        "second_question = \"How does data collection and analysis improve it?\"\n",
        "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
        "\n",
        "chat_history.extend(\n",
        "    [\n",
        "        HumanMessage(content=question),\n",
        "        AIMessage(content=ai_msg_2[\"answer\"]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Third question\n",
        "third_question = \"How can IUWM plans address financial challenges and integrate green infrastructure options?\"\n",
        "ai_msg_3 = rag_chain.invoke({\"input\": third_question, \"chat_history\": chat_history})\n",
        "\n",
        "# print(ai_msg_3[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_m6FrUnBs2PP"
      },
      "outputs": [],
      "source": [
        "# this simple function only returns the Q/A and not the context\n",
        "def extract_answers(raw_answer):\n",
        "    marker = \"<END CONTEXT>\"\n",
        "    if marker in raw_answer:\n",
        "        return raw_answer.split(marker)[-1]\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "# Example usage\n",
        "result = extract_answers(ai_msg_3[\"answer\"])\n",
        "print(result)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}